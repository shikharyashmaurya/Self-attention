{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Vectorisation","metadata":{}},{"cell_type":"markdown","source":"what is the most important requirement of natural language processing - ?\n\nhow to represent words into numbers (vectorisation)\n\ntechniques used are - \n\none hot encoding -(set of whole vocabulary is taken that particular word is represented with 1 and other with 0)\n\nbag of words - (represents sentence like OHE but the word is represented by the number of occurance of the word in the sentence)\n\nword embeddings","metadata":{}},{"cell_type":"markdown","source":"# Word embeddings","metadata":{}},{"cell_type":"markdown","source":"better than previous two techniques\n\nit captures the semantic meaning of the word (what does the word means)\n\na big chunk of data is passed through a neural network and it then represents every word with a n-dimensional vector\n\nthe n-values represent some qualities related to item (it is hard to define what they signify)\n\ntwo words with similar semantic meaning are represented with similar vectors\n\nin pictorial form it can be shown as (in a large dimensional space similar words will be closeer together like in a cluster)\n\nthus the position of word in space or vector shows what the word means (semantic meaning)\n\nthe word embeddings has a problem - average meaning","metadata":{}},{"cell_type":"markdown","source":"# average meaning","metadata":{}},{"cell_type":"markdown","source":"some words like apple represent both fruit and a comapny thus it becomes diffcult to define its position on vector space\n\nthus the position or vector values depends upon number of occurance on training dataset\n\nif in training dataset it is used more as a fruit its embedding will be similar to fruit\n\nthus the problem with word embeddings are they are static and do not change with context\n\nthus we need contextual embeddings\n\nthe mechanism to obtail contextual embeddings - self attention","metadata":{}},{"cell_type":"markdown","source":"# Self attention","metadata":{}},{"cell_type":"markdown","source":"self attention is like a function that can take static embeddings and convert it into a smart contextual embeddings (that understand word is used into which context)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}