{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# General contextual embedding","metadata":{}},{"cell_type":"markdown","source":"to convert static embedding into general contextual embedding\n\nwe do - \n\ncontextual embedding = weighted sum of similarity between static embedding of all words in \n\ncontextual embedding = sum(weights * static embedding)","metadata":{}},{"cell_type":"markdown","source":"example - \n\nsentence = money bank grows\n\nce(money) = w1 * se(money) + w2 * se(bank) + w3 * se(grows)\n\nhere\n\nw1 = similarity between static embedding vector of money and money (dot product between the vectors)\n\nw2 = similarity between static embedding vector of money and bank (dot product between the vectors)","metadata":{}},{"cell_type":"markdown","source":"# softmax the weight","metadata":{}},{"cell_type":"markdown","source":"for getting better results we will calculate the softmax of weights and then multiply them with static embeddings\n\nall operation are parallel operations thus scalable\n\nbut we do not have seqential information of sentence\n\nthere are no parameters (learning variable) involved","metadata":{}},{"cell_type":"markdown","source":"# Task specific contextual embedding","metadata":{}},{"cell_type":"markdown","source":"the general contextual embedding are fast and good but they do not have any learnable parameter\n\nthus they do not capture the semantic meaning of entire input data but only that of a single sentence\n\nthus to introduce parameters and get more semantic details - \n\nwe derive three diffrent vector (key, query and value) vector from embedding vector","metadata":{}},{"cell_type":"markdown","source":"# key, query and value","metadata":{}},{"cell_type":"markdown","source":"to get these three vectors we multiply our embedding vector with three diffrent matrix (Wa,Wb,Wc)\n\nthese are randomly initialised\n\nthen the value of matrix parameters are changed with backpropogation\n\nthus it is a learning parameter that learns with the input data\n\nthe value of matrix (Wa,Wb,Wc) remains same for all the words","metadata":{}},{"cell_type":"markdown","source":"# use of key, query and value","metadata":{}},{"cell_type":"markdown","source":"key , query and value vector are used for specific task-\n\nexample -\n\nlike in dictionary-\n\nkey value pair stores information\n\nquery looks for the key that is most similar to itself\n\ndictionary={1:a , 2:b , 3:c}\n\nhere 1,2,3 are keys and a,b,c are values\n\nsuppose we want to estimate the value of 1.014 (query)\n\nso we compare 1.104(query) with 1(key) they are similar, thus a(value) may be related to 1.104 (high weightage)\n\nthen we compare 1.104(query) with 2(key) they are less similar thus b(value) may be little related to 1.104 (medium weightage)\n\nkey and value is used for finding similarity (similarity between key of word and query)","metadata":{}},{"cell_type":"markdown","source":"in task specific contextual embedding \n\nembedding= sum(weights * values)\n\nweights = softmax(similarity between query(word) and key(all words of sentences))","metadata":{}}]}